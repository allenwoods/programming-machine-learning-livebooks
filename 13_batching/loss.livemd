# Loss vs. Accuracy

```elixir
Mix.install([
  {:exla, "~> 0.1.0-dev", github: "elixir-nx/nx", sparse: "exla"},
  {:nx, "~> 0.1.0-dev", github: "elixir-nx/nx", sparse: "nx", override: true},
  {:vega_lite, "~> 0.1.2"},
  {:kino, "~> 0.5.0"}
])

# Set the backend
Nx.Defn.global_default_options(compiler: EXLA)
```

## Section

## Load MNIST images

_The module to load MNIST data is the same used in the chapter 11, where `y_train` is already hot-encoded._

```elixir
defmodule C11.MNIST do
  @moduledoc """
  Use this Module to load the MNIST database (test, train, and labels).

  MNIST dataset specifications can be found here: http://yann.lecun.com/exdb/mnist/
  """

  @train_images_filename "./data/mnist/train-images-idx3-ubyte.gz"
  @test_images_filename "./data/mnist/t10k-images-idx3-ubyte.gz"
  @train_labels_filename "./data/mnist/train-labels-idx1-ubyte.gz"
  @test_labels_filename "./data/mnist/t10k-labels-idx1-ubyte.gz"

  @type t :: %__MODULE__{
          x_train: Nx.Tensor.t(),
          x_test: Nx.Tensor.t(),
          y_train: Nx.Tensor.t(),
          y_test: Nx.Tensor.t()
        }
  defstruct [:x_train, :x_test, :y_train, :y_test]

  @doc """
  Load the MNIST database and return the train and test images.

  `y_train` already hot-encoded.
  """
  @spec load() :: t()
  def load() do
    %__MODULE__{
      # 60000 images, each 784 elements (28 * 28 pixels)
      x_train: load_images(@train_images_filename),
      # 10000 images, each 784 elements, with the same structure as `x_train`
      x_test: load_images(@test_images_filename),
      # 60000 labels
      y_train: load_labels(@train_labels_filename) |> one_hot_encode(),
      # 10000 labels, with the same encoding as `y_train`
      y_test: load_labels(@test_labels_filename)
    }
  end

  @doc """
  One-hot encode the given tensor (classes: from 0 to 9).
  """
  @spec one_hot_encode(y :: Nx.Tensor.t()) :: Nx.Tensor.t()
  def one_hot_encode(y) do
    Nx.equal(y, Nx.tensor(Enum.to_list(0..9)))
  end

  @doc """
  Load the MNIST labels from the given file
  and return a matrix.
  """
  @spec load_labels(Path.t()) :: Nx.Tensor.t()
  def load_labels(filename) do
    # Open and unzip the file of labels
    with {:ok, binary} <- File.read(filename) do
      <<_::32, n_labels::32, labels_binary::binary>> = :zlib.gunzip(binary)

      # Create a tensor from the binary and
      # reshape the list of labels into a one-column matrix.
      labels_binary
      |> Nx.from_binary({:u, 8})
      |> Nx.reshape({n_labels, 1})
    end
  end

  @doc """
  Load the MNIST images from the given file
  and return a matrix.
  """
  @spec load_images(Path.t()) :: Nx.Tensor.t()
  def load_images(filename) do
    # Open and unzip the file of images
    with {:ok, binary} <- File.read(filename) do
      <<_::32, n_images::32, n_rows::32, n_cols::32, images_binary::binary>> =
        :zlib.gunzip(binary)

      # Create a tensor from the binary and
      # reshape the pixels into a matrix where each line is an image.
      images_binary
      |> Nx.from_binary({:u, 8})
      |> Nx.reshape({n_images, n_cols * n_rows})
    end
  end
end
```

### Load the data.

```elixir
# Use the public API to get train and test images
%{x_train: x_train, x_test: x_test, y_train: y_train, y_test: y_test} = data = C11.MNIST.load()
```

## Neural Network

_The classifier is the same developed in chapter 11._

```elixir
defmodule C11.Classifier do
  import Nx.Defn

  defn sigmoid(z) do
    Nx.divide(1, Nx.add(1, Nx.exp(Nx.negate(z))))
  end

  defn softmax(logits) do
    exponentials = Nx.exp(logits)

    Nx.divide(
      exponentials,
      Nx.sum(exponentials, axes: [1]) |> Nx.reshape({:auto, 1})
    )
  end

  defn sigmoid_gradient(sigmoid) do
    Nx.multiply(sigmoid, 1 - sigmoid)
  end

  defn loss(y, y_hat) do
    -Nx.sum(y * Nx.log(y_hat)) / elem(Nx.shape(y), 0)
  end

  defn prepend_bias(x) do
    bias = Nx.broadcast(1, {elem(Nx.shape(x), 0), 1})

    # Insert a column of 1s in the position 0 of x.
    # ("axis: 1" stands for: "insert a column, not a row")
    Nx.concatenate([bias, x], axis: 1)
  end

  defn forward(x, weight1, weight2) do
    h = sigmoid(Nx.dot(prepend_bias(x), weight1))
    y_hat = softmax(Nx.dot(prepend_bias(h), weight2))

    {y_hat, h}
  end

  defn back(x, y, y_hat, weight2, h) do
    w2_gradient =
      Nx.dot(
        Nx.transpose(prepend_bias(h)),
        Nx.subtract(y_hat, y)
      ) / elem(Nx.shape(x), 0)

    w1_gradient =
      Nx.dot(
        Nx.transpose(prepend_bias(x)),
        Nx.dot(y_hat - y, Nx.transpose(weight2[1..-1//1])) * sigmoid_gradient(h)
      ) / elem(Nx.shape(x), 0)

    {w1_gradient, w2_gradient}
  end

  defn classify(x, weight1, weight2) do
    {y_hat, _h} = forward(x, weight1, weight2)
    labels = Nx.argmax(y_hat, axis: 1)
    Nx.reshape(labels, {:auto, 1})
  end

  defn initialize_weights(opts \\ []) do
    opts = keyword!(opts, [:w1_shape, :w2_shape])

    mean = 0.0
    std_deviation = 0.01
    weight1 = Nx.random_normal(opts[:w1_shape], mean, std_deviation)
    weight2 = Nx.random_normal(opts[:w2_shape], mean, std_deviation)

    {weight1, weight2}
  end

  def report(iteration, x_train, y_train, x_test, y_test, weight1, weight2) do
    {y_hat, _h} = forward(x_train, weight1, weight2)
    training_loss = loss(y_train, y_hat) |> Nx.to_number()
    classifications = classify(x_test, weight1, weight2)
    accuracy = Nx.multiply(Nx.mean(Nx.equal(classifications, y_test)), 100.0) |> Nx.to_number()

    IO.puts("Iteration #{iteration}, Loss: #{training_loss}, Accuracy: #{accuracy}%")

    {training_loss, accuracy}
  end

  def train(x_train, y_train, x_test, y_test, n_hidden_nodes, iterations, lr) do
    n_input_variables = elem(Nx.shape(x_train), 1)
    n_classes = elem(Nx.shape(y_train), 1)

    {initial_weight_1, initial_weight_2} =
      initialize_weights(
        w1_shape: {n_input_variables + 1, n_hidden_nodes},
        w2_shape: {n_hidden_nodes + 1, n_classes}
      )

    initial_loss_history = []
    initial_accuracy_history = []

    initial_accumulator = {
      {initial_weight_1, initial_weight_2},
      {initial_loss_history, initial_accuracy_history}
    }

    Enum.reduce(0..(iterations - 1), initial_accumulator, fn i,
                                                             {{w1, w2},
                                                              {loss_hist, accuracy_hist}} ->
      {updated_w1, updated_w2} = step(x_train, y_train, w1, w2, lr)
      {loss, accuracy} = report(i, x_train, y_train, x_test, y_test, updated_w1, updated_w2)

      {{updated_w1, updated_w2}, {loss_hist ++ [loss], accuracy_hist ++ [accuracy]}}
    end)
  end

  defnp step(x_train, y_train, w1, w2, lr) do
    {y_hat, h} = forward(x_train, w1, w2)
    {w1_gradient, w2_gradient} = back(x_train, y_train, y_hat, w2, h)
    w1 = w1 - w1_gradient * lr
    w2 = w2 - w2_gradient * lr

    {w1, w2}
  end
end
```

### Loss vs Accuracy - 30 iterations

```elixir
hidden_nodes = 200
iterations = 30
learning_rate = 0.01

{{_w1, _w2}, {loss_history, accuracy_history}} =
  C11.Classifier.train(
    x_train,
    y_train,
    x_test,
    y_test,
    hidden_nodes,
    iterations,
    learning_rate
  )
```

```elixir
# 30 iterations

alias VegaLite, as: Vl

loss_inputs =
  loss_history
  |> Enum.with_index(1)
  |> Enum.map(fn {loss, iteration} -> %{loss: loss, iteration: iteration} end)

accuracy_inputs =
  accuracy_history
  |> Enum.with_index(1)
  |> Enum.map(fn {accuracy, iteration} -> %{accuracy: accuracy, iteration: iteration} end)

Vl.new(width: 600, height: 400)
|> Vl.concat([
  Vl.new()
  |> Vl.data_from_values(loss_inputs)
  |> Vl.mark(:line)
  |> Vl.encode_field(:x, "iteration", type: :quantitative)
  |> Vl.encode_field(:y, "loss", type: :quantitative),
  Vl.new()
  |> Vl.data_from_values(accuracy_inputs)
  |> Vl.mark(:line)
  |> Vl.encode_field(:x, "iteration", type: :quantitative)
  |> Vl.encode_field(:y, "accuracy", type: :quantitative)
])
```

### Loss vs Accuracy - 1000 iterations

```elixir
# It may take some minutes
hidden_nodes = 200
iterations = 1000
learning_rate = 0.01

{{_w1, _w2}, {loss_history, accuracy_history}} =
  C11.Classifier.train(
    x_train,
    y_train,
    x_test,
    y_test,
    hidden_nodes,
    iterations,
    learning_rate
  )
```

```elixir
# 1000 iterations

alias VegaLite, as: Vl

loss_inputs =
  loss_history
  |> Enum.with_index(1)
  |> Enum.map(fn {loss, iteration} -> %{loss: loss, iteration: iteration} end)

accuracy_inputs =
  accuracy_history
  |> Enum.with_index(1)
  |> Enum.map(fn {accuracy, iteration} -> %{accuracy: accuracy, iteration: iteration} end)

Vl.new(width: 600, height: 400)
|> Vl.concat([
  Vl.new()
  |> Vl.data_from_values(loss_inputs)
  |> Vl.mark(:line)
  |> Vl.encode_field(:x, "iteration", type: :quantitative)
  |> Vl.encode_field(:y, "loss", type: :quantitative),
  Vl.new()
  |> Vl.data_from_values(accuracy_inputs)
  |> Vl.mark(:line)
  |> Vl.encode_field(:x, "iteration", type: :quantitative)
  |> Vl.encode_field(:y, "accuracy", type: :quantitative)
])
```
