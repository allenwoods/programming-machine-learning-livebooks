# MNIST 3 sets

```elixir
Mix.install([
  {:exla, "~> 0.3"},
  {:nx, "~> 0.3"},
  {:kino, "~> 0.6.2"},
  {:kino_vega_lite, "~> 0.1.3"},
  {:vega_lite, "~> 0.1.6"}
])

# Set the backend
Nx.Defn.global_default_options(compiler: EXLA)
```

## Load MNIST images

_The module to load MNIST data is the based on the one used in the chapter 11._

```elixir
defmodule C14.MNIST do
  @moduledoc """
  Use this Module to load the MNIST database (test, train and validation sets).

  MNIST dataset specifications can be found here: http://yann.lecun.com/exdb/mnist/
  """

  @train_images_filename "./data/mnist/train-images-idx3-ubyte.gz"
  @test_images_filename "./data/mnist/t10k-images-idx3-ubyte.gz"
  @train_labels_filename "./data/mnist/train-labels-idx1-ubyte.gz"
  @test_labels_filename "./data/mnist/t10k-labels-idx1-ubyte.gz"

  @type t :: %__MODULE__{
          x_train: Nx.Tensor.t(),
          x_test: Nx.Tensor.t(),
          x_validation: Nx.Tensor.t(),
          y_train: Nx.Tensor.t(),
          y_test: Nx.Tensor.t(),
          y_validation: Nx.Tensor.t(),
          y_train_unencoded: Nx.Tensor.t()
        }
  defstruct [
    :x_train,
    :x_test,
    :x_validation,
    :y_train,
    :y_test,
    :y_validation,
    :y_train_unencoded
  ]

  @doc """
  Load the MNIST database and return the train, test and validation images.

  `y_train` already hot-encoded.
  """
  @spec load() :: t()
  def load() do
    # 10000 images, each 784 elements, with the same structure as `x_train`
    x_test_all = load_images(@test_images_filename)
    {x_validation, x_test} = split(x_test_all)

    # 10000 labels, each a single digit from 0 to 9
    y_test_all = load_labels(@test_labels_filename)
    {y_validation, y_test} = split(y_test_all)

    %__MODULE__{
      # 60000 images, each 784 elements (28 * 28 pixels)
      x_train: load_images(@train_images_filename),
      # 5000 images, each 784 elements, with the same structure as `x_train`
      x_validation: x_validation,
      # 5000 images, each 784 elements, with the same structure as `x_train`
      x_test: x_test,
      # 60000 labels, each consisting of 10 one-hot encoded elements
      y_train: load_labels(@train_labels_filename) |> one_hot_encode(),
      # 5000 labels, each a single digit from 0 to 9
      y_validation: y_validation,
      # 5000 labels, each a single digit from 0 to 9
      y_test: y_test,
      y_train_unencoded: load_labels(@train_labels_filename)
    }
  end

  defp split(tensor) do
    {x, _} = Nx.shape(tensor)
    len = trunc(x / 2)
    first_half = Nx.slice_along_axis(tensor, 0, len, axis: 0)
    second_half = Nx.slice_along_axis(tensor, len + 1, len, axis: 0)
    {first_half, second_half}
  end

  @doc """
  One-hot encode the given tensor (classes: from 0 to 9).
  """
  @spec one_hot_encode(y :: Nx.Tensor.t()) :: Nx.Tensor.t()
  def one_hot_encode(y) do
    Nx.equal(y, Nx.tensor(Enum.to_list(0..9)))
  end

  @doc """
  Load the MNIST labels from the given file
  and return a matrix.
  """
  @spec load_labels(Path.t()) :: Nx.Tensor.t()
  def load_labels(filename) do
    # Open and unzip the file of labels
    with {:ok, binary} <- File.read(filename) do
      <<_::32, n_labels::32, labels_binary::binary>> = :zlib.gunzip(binary)

      # Create a tensor from the binary and
      # reshape the list of labels into a one-column matrix.
      labels_binary
      |> Nx.from_binary({:u, 8})
      |> Nx.reshape({n_labels, 1})
    end
  end

  @doc """
  Load the MNIST images from the given file
  and return a matrix.
  """
  @spec load_images(Path.t()) :: Nx.Tensor.t()
  def load_images(filename) do
    # Open and unzip the file of images
    with {:ok, binary} <- File.read(filename) do
      <<_::32, n_images::32, n_rows::32, n_cols::32, images_binary::binary>> =
        :zlib.gunzip(binary)

      # Create a tensor from the binary and
      # reshape the pixels into a matrix where each line is an image.
      images_binary
      |> Nx.from_binary({:u, 8})
      |> Nx.reshape({n_images, n_cols * n_rows})
    end
  end
end
```

### Load the data

```elixir
%{
  x_train: x_train,
  x_test: x_test,
  x_validation: x_validation,
  y_train: y_train,
  y_test: y_test,
  y_validation: y_validation,
  y_train_unencoded: y_train_unencoded
} = C14.MNIST.load()
```

## Training Neural Network

_The classifier is based on the one implemented in Chapter 12._

```elixir
defmodule C14.NeuralNetwork do
  import Nx.Defn

  defn sigmoid(z) do
    Nx.divide(1, Nx.add(1, Nx.exp(Nx.negate(z))))
  end

  defn softmax(logits) do
    exponentials = Nx.exp(logits)

    Nx.divide(
      exponentials,
      Nx.sum(exponentials, axes: [1]) |> Nx.reshape({:auto, 1})
    )
  end

  defn sigmoid_gradient(sigmoid) do
    Nx.multiply(sigmoid, 1 - sigmoid)
  end

  defn loss(y, y_hat) do
    -Nx.sum(y * Nx.log(y_hat)) / elem(Nx.shape(y), 0)
  end

  defn prepend_bias(x) do
    bias = Nx.broadcast(1, {elem(Nx.shape(x), 0), 1})

    Nx.concatenate([bias, x], axis: 1)
  end

  defn forward(x, weight1, weight2) do
    h = sigmoid(Nx.dot(prepend_bias(x), weight1))
    y_hat = softmax(Nx.dot(prepend_bias(h), weight2))

    {y_hat, h}
  end

  defn back(x, y, y_hat, weight2, h) do
    w2_gradient =
      Nx.dot(
        Nx.transpose(prepend_bias(h)),
        Nx.subtract(y_hat, y)
      ) / elem(Nx.shape(x), 0)

    w1_gradient =
      Nx.dot(
        Nx.transpose(prepend_bias(x)),
        Nx.dot(y_hat - y, Nx.transpose(weight2[1..-1//1])) * sigmoid_gradient(h)
      ) / elem(Nx.shape(x), 0)

    {w1_gradient, w2_gradient}
  end

  defn classify(x, weight1, weight2) do
    {y_hat, _h} = forward(x, weight1, weight2)
    labels = Nx.argmax(y_hat, axis: 1)
    Nx.reshape(labels, {:auto, 1})
  end

  defn initialize_weights(opts \\ []) do
    opts = keyword!(opts, [:w1_shape, :w2_shape])

    mean = 0.0
    std_deviation = 0.01
    weight1 = Nx.random_normal(opts[:w1_shape], mean, std_deviation)
    weight2 = Nx.random_normal(opts[:w2_shape], mean, std_deviation)

    {weight1, weight2}
  end

  defp report(iteration, training_loss, test_loss, validation_loss) do
    IO.inspect(
      "#{iteration} > Training loss: #{Nx.to_number(training_loss)}% - Test loss: #{Nx.to_number(test_loss)}% - Validation loss: #{Nx.to_number(validation_loss)}%"
    )
  end

  def accuracy(x_test, y_test, w1, w2) do
    classifications = classify(x_test, w1, w2)
    Nx.multiply(Nx.mean(Nx.equal(classifications, y_test)), 100.0) |> Nx.to_number()
  end

  def train(
        x_train,
        y_train,
        x_test,
        y_test,
        x_validation,
        y_validation,
        n_hidden_nodes,
        iterations,
        lr
      ) do
    n_input_variables = elem(Nx.shape(x_train), 1)
    n_classes = elem(Nx.shape(y_train), 1)

    {initial_weight_1, initial_weight_2} =
      initialize_weights(
        w1_shape: {n_input_variables + 1, n_hidden_nodes},
        w2_shape: {n_hidden_nodes + 1, n_classes}
      )

    initial_acc = %{
      w1: initial_weight_1,
      w2: initial_weight_2,
      training_losses: [],
      test_losses: [],
      validation_losses: []
    }

    Enum.reduce(0..(iterations - 1), initial_acc, fn i, %{w1: w1, w2: w2} = state ->
      {updated_w1, updated_w2, training_loss, test_loss, validation_loss} =
        step(x_train, y_train, x_test, y_test, x_validation, y_validation, w1, w2, lr)

      report(i, training_loss, test_loss, validation_loss)

      %{
        w1: updated_w1,
        w2: updated_w2,
        training_losses: state.training_losses ++ [Nx.to_number(training_loss)],
        test_losses: state.test_losses ++ [Nx.to_number(test_loss)],
        validation_losses: state.validation_losses ++ [Nx.to_number(validation_loss)]
      }
    end)
  end

  defnp step(x_train, y_train, x_test, y_test, x_validation, y_validation, w1, w2, lr) do
    {y_hat_train, h} = forward(x_train, w1, w2)
    {y_hat_test, _h} = forward(x_test, w1, w2)
    {y_hat_validation, _h} = forward(x_validation, w1, w2)

    {w1_gradient, w2_gradient} = back(x_train, y_train, y_hat_train, w2, h)
    w1 = w1 - w1_gradient * lr
    w2 = w2 - w2_gradient * lr

    training_loss = loss(y_train, y_hat_train)
    test_loss = loss(y_test, y_hat_test)
    validation_loss = loss(y_validation, y_hat_validation)

    {w1, w2, training_loss, test_loss, validation_loss}
  end
end
```

```elixir
hidden_nodes = 200
learning_rate = 0.01
iterations = 10_000

# The `train/9` function stores the loss for both training, test and validation sets
# at each step.
# Differently from the other `train` functions in previous livebooks, it expects
# _both_ `y_train`, `y_test` and `y_validation` to be one hot encoded.
%{
  w1: w1,
  w2: w2,
  training_losses: training_losses,
  test_losses: test_losses,
  validation_losses: validation_losses
} =
  C14.NeuralNetwork.train(
    x_train,
    y_train,
    x_test,
    C14.MNIST.one_hot_encode(y_test),
    x_validation,
    C14.MNIST.one_hot_encode(y_validation),
    hidden_nodes,
    iterations,
    learning_rate
  )
```

```elixir
training_accuracy = C14.NeuralNetwork.accuracy(x_train, y_train_unencoded, w1, w2)
test_accuracy = C14.NeuralNetwork.accuracy(x_test, y_test, w1, w2)
validation_accuracy = C14.NeuralNetwork.accuracy(x_validation, y_validation, w1, w2)

IO.inspect(
  "Training accuracy: #{training_accuracy} % - Test accuracy: #{test_accuracy} % - Validation accuracy: #{validation_accuracy} %"
)

:ok
```

```elixir
alias VegaLite, as: Vl

iterations = Enum.to_list(0..(length(test_losses) - 1))

training_losses_inputs =
  Enum.zip_with([training_losses, iterations], fn [l, i] ->
    %{loss: l, iteration: i, type: "training loss"}
  end)

test_losses_inputs =
  Enum.zip_with([test_losses, iterations], fn [l, i] ->
    %{loss: l, iteration: i, type: "test loss"}
  end)

validation_losses_inputs =
  Enum.zip_with([validation_losses, iterations], fn [l, i] ->
    %{loss: l, iteration: i, type: "validation loss"}
  end)

Vl.new(width: 600, height: 400)
|> Vl.layers([
  Vl.new()
  |> Vl.data_from_values(training_losses_inputs)
  |> Vl.mark(:line)
  |> Vl.encode_field(:x, "iteration", type: :quantitative)
  |> Vl.encode_field(:y, "loss", type: :quantitative)
  |> Vl.encode(:color, field: "type"),
  Vl.new()
  |> Vl.data_from_values(test_losses_inputs)
  |> Vl.mark(:line)
  |> Vl.encode_field(:x, "iteration", type: :quantitative)
  |> Vl.encode_field(:y, "loss", type: :quantitative)
  |> Vl.encode(:color, field: "type"),
  Vl.new()
  |> Vl.data_from_values(validation_losses_inputs)
  |> Vl.mark(:line)
  |> Vl.encode_field(:x, "iteration", type: :quantitative)
  |> Vl.encode_field(:y, "loss", type: :quantitative)
  |> Vl.encode(:color, field: "type")
])
```
